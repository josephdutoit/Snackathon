# -*- coding: utf-8 -*-
"""lab8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZvlV-AFQPwiDU5JrJqy_ruGIKQrkBg8K

<a
href="https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab10.ipynb"
  target="_parent">
  <img
    src="https://colab.research.google.com/assets/colab-badge.svg"
    alt="Open In Colab"/>
</a>

# Lab 8: Transfer Learning/Fine-Tuning

## Description

### Objective

- Gain experience fine-tuning pre-trained models to domain-specific applications.

### Deliverable

For this lab you will submit an ipython notebook via learning suite. The bulk of the work is in modifying fine-tuning a pre-trained ResNet. Fine-tuning the GPT-2 language model is pretty easy. The provided code works as is; you will just have to swap in your own text dataset.

### Grading

- 35% Create a dataset class for your own dataset
- 35% Create a network class that wraps a pretrained ResNet
- 20% Implement unfreezing in the network class
- 10% Fine-tune GPT-2 on your own dataset

### Tips
- Your life will be better if you download a dataset that already has the data in the expected format for ImageFolder (make sure to read the documentation!). The datasets recommended below are in the correct format.
- Get the CNN working on the provided dataset (bird species classification) before swapping in your own.
- For reference on freezing/unfreezing network weights, see [this github gist](https://gist.github.com/jcjohnson/6e41e8512c17eae5da50aebef3378a4c)
- Check PyTorch's documentation to learn the difference between `requires_grad=False` and `requires_grad_(False)`.
- For training GPT-2, first try the medium-size (355M parameter) model. If your Colab instance doesn't have enough GPU space, you may need to switch to the small-size (124M parameter) model, but the results will be less impressive.
"""

# !pip install -q torch torchvision
# !nvidia-smi
from torchvision.models import resnet152
from torchvision import transforms, datasets
from torch.utils.data import DataLoader, Dataset
import torch
from torch import optim, nn
import zipfile
# from google.colab import files
import os
import sys
from PIL import Image, ImageOps
from tqdm import tqdm
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import gc

"""## 1 Fine-tune a ResNet for image classification

### 1.1 Find a dataset to fine-tune on, and make a Dataset class (1 hr.)

#### TODO:

- Inherit from torch.utils.data.Dataset
- Use a [torchvision.datasets.ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder)
- Don't spend too long finding another dataset. Some suggestions that you are free to use:
 - https://www.kaggle.com/jessicali9530/stanford-dogs-dataset
 - https://www.kaggle.com/puneet6060/intel-image-classification

#### Help for downloading kaggle datasets
Downloading Kaggle datasets requires authentication, so you can't just download from a url. Here are some step-by-step instructions of how to get Kaggle datasets in Colab

1. Create an API key in Kaggle
    - Click on profile photo
    - Go to 'My Account'
    - Scroll down to the API access section and click "Create New API Token"
    - `kaggle.json` is now downloaded to your computer

2. Upload the API key and install the Kaggle API client by running the next cell (run it again if it throws an error the first time). Also, `files.upload()` may not work in Firefox. One solution is to expand the Files banner (indicated by the '>' tab on the left side of the page) and use that to upload the key.
"""

# # Run this cell and select the kaggle.json file downloaded
# # from the Kaggle account settings page.
# from google.colab import files
# files.upload()
# # Next, install the Kaggle API client.
# !pip install -q kaggle
# # Let's make sure the kaggle.json file is present.
# !ls -lha kaggle.json
# # The Kaggle API client expects this file to be in ~/.kaggle,
# # so move it there.
# !mkdir -p ~/.kaggle
# !mv kaggle.json ~/.kaggle/
# # This permissions change avoids a warning on Kaggle tool startup.
# !chmod 600 ~/.kaggle/kaggle.json

"""3. Copy the desired dataset locally"""

# Example download command for dataset found here: https://www.kaggle.com/akash2907/bird-species-classification
# !kaggle datasets download -d akash2907/bird-species-classification

"""#### Make the Dataset class
See the implementation below for reference, and implement a dataset class for the dataset you choose.
"""

# class BirdDataset(Dataset):
#     def __init__(self, zip_file='bird-species-classification.zip', size=256, train=True, upload=False):
#         super(BirdDataset, self).__init__()

#         self.train = train
#         extract_dir = os.path.splitext(zip_file)[0]
#         if not os.path.exists(extract_dir):
#             os.makedirs(extract_dir)
#             self.extract_zip(zip_file, extract_dir)
#             # Resize the images - originally they are high resolution. We could do this
#             # in the DataLoader, but it will read the full-resolution files from disk
#             # every time before resizing them, making training slow
#             self.resize(extract_dir, size=size)

#         postfix = 'train' if train else 'test'

#         if train:
#             # The bird-species dataset mistakenly has a train_data folder inside of train_data
#             self.dataset_folder = datasets.ImageFolder(os.path.join(extract_dir, 'train_data', 'train_data'), transform=transforms.Compose([transforms.ToTensor()]))
#         else:
#             self.dataset_folder = datasets.ImageFolder(os.path.join(extract_dir, 'test_data', 'test_data'), transform=transforms.Compose([transforms.ToTensor()]))

#     def extract_zip(self, zip_file, extract_dir):
#         print("Extracting", zip_file)
#         with zipfile.ZipFile(zip_file, 'r') as zip_ref:
#             zip_ref.extractall(extract_dir)

#     def resize(self, path, size=256):
#         """Resizes all images in place"""
#         print("Resizing images")
#         dirs = os.walk(path)
#         for root, dirs, files in os.walk(path):
#             for item in files:
#                 name = os.path.join(root, item)
#                 if os.path.isfile(name):
#                     im = Image.open(name)
#                     im = ImageOps.fit(im, (size, size))
#                     im.save(name[:-3] + 'bmp', 'BMP')
#                     os.remove(name)

#     def __getitem__(self, i):
#         return self.dataset_folder[i]

#     def __len__(self):
#         return len(self.dataset_folder)

# bird_data = BirdDataset()

# from google.colab import drive
import yaml
# drive.mount('/content/drive')

# !cp /content/drive/MyDrive/hackathon_data.zip /content
# !unzip /content/hackathon_data.zip

with open('bev_classification/names.yaml', 'r') as f:
    LABEL_LIST = yaml.full_load(f)['classes']

LABEL_DICT = dict([(LABEL_LIST[i], i) for i in range(len(LABEL_LIST))])

#########################
# Implement your own Dataset
#########################

class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, transform=None, train=False):
        self.transform = transform
        self.data_dir = '/home/jcdutoit/Snackathon/bev_classification'
        # self.data_labels = os.path.join(self.data_dir, "/datasets/train.txt")
        if train:
          self.data_labels = "/home/jcdutoit/Snackathon/bev_classification/datasets/train.txt"
          raw_txt = np.loadtxt(self.data_labels, dtype=str)
          lines_labels = np.array([line.split(',') for line in raw_txt])
        else:
          self.data_labels = "/home/jcdutoit/Snackathon/bev_classification/datasets/test_edited.txt"
          raw_txt = np.loadtxt(self.data_labels, dtype=str)
          lines_labels = np.array([[line, line.split('/')[2]] for line in raw_txt])

        self.df = pd.DataFrame(lines_labels, columns=['image', 'label'])
        self.transform = transforms.Compose([transforms.Resize(256),
                                             transforms.CenterCrop(224),
                                             transforms.PILToTensor()])

                                            #  transforms.Resize(256),
                                            #  transforms.CenterCrop(224),
                                            #  transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])

        # TODO Augment Data with Transforms
        #     transforms.Resize(256),
        #     transforms.CenterCrop(224),
        #     transforms.ToTensor])


    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        # Get the image and label from the file
        img_path = os.path.join(self.data_dir, self.df.iloc[idx, 0])
        image = Image.open(img_path)
        label = LABEL_DICT[self.df.iloc[idx, 1]]
        l_tensor = torch.tensor(label)
        # Convert the label to one hot encoding
        # one_hot = torch.zeros(len(self.label_dict))
        # one_hot[label] = 1.0


        # Transform the image
        image_t = self.transform(image).float()
        return image_t, l_tensor

"""### 1.2 Wrap a pretrained ResNet in an `nn.Module` (30 min)

#### TODO:

- Make a model class that inherits from `nn.Module`
- Wrap a pretrained ResNet and swap out the last layer of that network with a layer that maps to the number of classes in your new dataset

#### Make your model class
"""

class CustomResNet(nn.Module):
    def __init__(self, num_classes, start_frozen=False):
        super(CustomResNet, self).__init__()

        # Part 1.2
        # Load the model - make sure it is pre-trained
        self.res_model = resnet152(pretrained=True)

        # Part 1.4
        if start_frozen:
            # Turn off all gradients of the resnet
            for param in self.res_model.parameters():
                param.requires_grad = False

        # Part 1.2
        # Look at the code of torchvision.models.resnet152 (or print the ResNet object) to find the name of the attribute to override (the last layer of the ResNet)
        # Override the output linear layer of the neural network to map to the correct number of classes. Note that this new layer has requires_grad = True
        self.res_model.fc = nn.Linear(self.res_model.fc.in_features, 99)

    def unfreeze(self, n_layers):
        # Part 1.4
        # Turn on gradients for the last n_layers

        child_list = list(self.res_model.children())
        for child in child_list[len(child_list) - n_layers - 1:]:
           for param in child.parameters():
               param.requires_grad = True

    def forward(self, x):
        # Part 1.2
        # Pass x through the resnet
        return self.res_model(x)

"""### 1.3 Read through and run this training loop"""

def accuracy(y_hat, y_truth):
    """Gets average accuracy of a vector of predictions"""

    preds = torch.argmax(y_hat, dim=1)
    acc = torch.mean((preds == y_truth).float())
    return acc

def evaluate(model, objective, val_loader, device, epoch):
    """Gets average accuracy and loss for the validation set"""

    val_losses = 0
    val_accs = 0
    batches = 0
    # model.eval() so that batchnorm and dropout work in eval mode
    model.eval()
    # torch.no_grad() to turn off computation graph creation. This allows for temporal
    # and spatial complexity improvements, which allows for larger validation batch
    # sizes so it’s recommended
    preds = []
    top5_preds = []
    with torch.no_grad():
        for x, y_truth in val_loader:

            batches += 1

            x, y_truth = x.to(device), y_truth.to(device)
            y_hat = model(x)
            val_loss = objective(y_hat, y_truth)
            val_acc = accuracy(y_hat, y_truth)
            preds.append(LABEL_LIST[int(torch.argmax(y_hat, dim=1).item())])

            # Do top 5
            _, ind = y_hat.topk(5, dim=1, largest=True)
            ind = ind.tolist()[0]
            top5_preds.append([LABEL_LIST[int(ind[i])] for i in range(len(ind))])

            val_losses += val_loss.item()
            val_accs += val_acc

    print("Writing to /home/jcdutoit/Snackathon/val_" + str(epoch) + ".txt'")
    with open('/home/jcdutoit/Snackathon/val_'+str(epoch)+'.txt', 'w') as f:
        for pred in preds:
            f.write(pred + '\n')

    with open('/home/jcdutoit/Snackathon/top5_val_'+str(epoch)+'.txt', 'w') as f:
        for idx in top5_preds:
            pred_string = ', '.join(idx)
            f.write(pred_string + '\n')

    model.train()

    return val_losses/batches, val_accs/batches

def train(start_frozen=False, model_unfreeze=0):
    """Fine-tunes a CNN
    Args:
        start_frozen (bool): whether to start with the network weights frozen.
        model_unfreeze (int): the maximum number of network layers to unfreeze
    """

    gc.collect()
    epochs = 5
    # Start with a very low learning rate
    lr = .00005
    val_every = 500
    num_classes = 16
    batch_size = 32
    device = torch.device('cuda:0')

    # Data
    # TODO: Use your own dataset
    # train_dataset = CustomDataset(upload=True, train=True)
    # val_dataset = CustomDataset(upload=True, train=False)

    train_dataset = CustomDataset(train=True)
    val_dataset = CustomDataset(train=False)

    train_loader = DataLoader(train_dataset,
                              shuffle=True,
                              num_workers=8,
                              batch_size=batch_size)
    val_loader = DataLoader(val_dataset,
                              shuffle=False,
                              num_workers=8,
                              batch_size=1)

    # Model
    model = CustomResNet(num_classes, start_frozen=start_frozen).to(device)

    # Objective
    objective = nn.CrossEntropyLoss()
    # Optimizer
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-1)

    # Progress bar
    pbar = tqdm(total=len(train_loader) * epochs)

    train_losses = []
    train_accs = []
    val_losses = []
    val_accs = []

    cnt = 0
    for epoch in range(epochs):
        print("Epoch: ", epoch)
        # Implement model unfreezing
        if cnt < model_unfreeze and cnt % 45 == 0:
            # Part 1.4
            # Unfreeze the last layers, one more each epoch
            layers = int(cnt / 45)
            print("\nUnfreezing " + str(layers) + " layers")
            model.unfreeze(layers)

        for x, y_truth in train_loader:

            x, y_truth = x.to(device), y_truth.to(device)

            optimizer.zero_grad()

            y_hat = model(x)
            train_loss = objective(y_hat, y_truth)
            train_acc = accuracy(y_hat, y_truth)

            train_loss.backward()
            optimizer.step()

            train_accs.append(train_acc)
            train_losses.append(train_loss.item())

            if cnt % val_every == 0:
                val_loss, val_acc = evaluate(model, objective, val_loader, device, cnt)
                val_losses.append(val_loss)
                val_accs.append(val_acc)
                torch.save(model, '/home/jcdutoit/Snackathon/' + str(epoch) + '_snack_model.pt')

            # pbar.set_description('train loss:{:.4f}, train accuracy:{:.4f}.'.format(train_loss.item(), train_acc))
            pbar.set_description('train loss:{:.4f}, train accuracy:{:.4f}, val loss:{:.4f}, val accuracy:{:.4f}.'.format(train_loss.item(), train_acc, val_losses[-1], val_accs[-1]))
            pbar.update(1)
            cnt += 1

    pbar.close()

train(start_frozen=False, model_unfreeze=0)

